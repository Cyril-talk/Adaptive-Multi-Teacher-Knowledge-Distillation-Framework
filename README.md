# Adaptive-Multi-Teacher-Knowledge-Distillation-Framework
Adaptive Multi-Teacher Knowledge Distillation Framework with Foundation Models for Medical Image Analysis
# Adaptive-Multi-Teacher-Knowledge-Distillation-Framework

A multi-teacher knowledge distillation framework for medical image segmentation, classification, and recognition tasks.

## âš ï¸ Project Status

> **Note**: This repository is currently under active development. The code structure is being continuously refined and reorganized for better clarity and maintainability. We appreciate your patience and welcome any feedback or contributions.

### Current Status
- âœ… Core functionality implemented
- âœ… Multi-teacher distillation framework working
- âœ… Support for 6 medical datasets
- ğŸ”„ Code structure optimization in progress
- ğŸ”„ Documentation being improved
- ğŸ“‹ Comprehensive refactoring planned

### Upcoming Improvements
- [ ] Unified code structure
- [ ] Modular architecture redesign
- [ ] Enhanced documentation
- [ ] Code style standardization
- [ ] Unit tests and CI/CD pipeline
- [ ] Performance benchmark

---

## ğŸ¯ Overview

This project implements a novel multi-teacher knowledge distillation framework for medical image analysis. The framework leverages multiple pre-trained foundation models (MedSAM, USFM, RETFound, BioMedParse) to distill knowledge into a lightweight student model, achieving competitive performance across various medical imaging tasks.

### Key Innovations
- **Dynamic Teacher Gating**: Automatically adjusts teacher contributions based on task relevance
- **Cross-Task Knowledge Transfer**: Enables knowledge sharing across different medical imaging domains
- **Heterogeneous Distillation**: Handles different teacher architectures and output formats
- **Adaptive Loss Weighting**: Dynamically balances multiple distillation objectives

---

## âœ¨ Features

- ğŸ¥ **Multi-Task Support**: Segmentation, classification, and recognition
- ğŸ‘¨â€ğŸ« **Multiple Teacher Models**: MedSAM, USFM, RETFound_MAE, BioMedParse
- ğŸ“ **Advanced Distillation**: Feature-level, output-level, and contrastive distillation
- ğŸ”„ **Dynamic Gating**: Automatic teacher weight adjustment
- ğŸ“Š **Comprehensive Metrics**: Task-specific evaluation metrics


Dataset download pathï¼š
- BUSIï¼šhttps://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset
Kvasir-segï¼šhttps://datasets.simula.no/kvasir-seg/
COVIDï¼šhttps://aistudio.baidu.com/datasetdetail/127908
MSD-heartï¼šhttps://aistudio.baidu.com/datasetdetail/23911
APTOS2019ï¼šhttps://www.kaggle.com/datasets/mariaherrerot/aptos2019/data
ISIC2017ï¼šhttps://aistudio.baidu.com/datasetdetail/65747

Weight file download path:
USFMï¼šhttps://github.com/openmedlab/USFM
MedSAMï¼šhttps://huggingface.co/wanglab/medsam-vit-base
RETFoundï¼šhttps://huggingface.co/RETFound/RETFound
BiomedParseï¼šhttps://huggingface.co/microsoft/BiomedParse



